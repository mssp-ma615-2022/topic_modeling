---
title: "IMDB topic modeling"
output: pdf_document
author: "Zhi Tu, Tao Guo, Yalong Wang, Tianjian Xie"
date: "2022-11-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, tidytext, ggplot2, tm, topicmodels)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(tm)
library(topicmodels)
```

## Read data

```{r}
imdb <- read.csv("IMDB Dataset.csv")
```

## Clean data

```{r warning=FALSE}
# Dataset separating
# separate by html format

reviews <- imdb %>% 
  mutate(review_number = row_number()) %>%
  separate(review,c("1", "2", "3", "4", "5","6"), sep="<br /><br />", convert = TRUE) %>% 
  pivot_longer(c("1", "2", "3", "4","5","6"), names_to = "lines", names_transform = list(lines = as.integer), values_to = "text") %>%
  arrange(review_number, lines) %>%
  relocate(text) %>%
  tibble()

# separate by words

tidy_reviews <- reviews %>% 
  unnest_tokens(word, text) 
```

# Building stop words

```{r}
# custom stop words
data(stop_words)
custom_stop_words <- bind_rows(tibble(word = c("movie", "film", "movies", "time","story","plot","films","director","watch","characters", "watching","watched", "scenes", "tv","scene","series", "found","fan","lot","dvd","book","2","minutes","read","feel","makes","sense","script","based","genre","cast","role","performance","john","played","play","actors","character","production","bit","line","main","excellent","wonderful","called","10","1","remember","version","half","episode","ago","worst","budget","low","special","actor","michael","playing","set","3","heard","reviews","boring","idea","absolutely","NA","nice","screen","directed","plays","released","james","robert","david","top","actress","acting","completely","stupid","original","roles","picture","audience", "viewer","supporting","recommend"), 
                                      lexicon = c("custom")), 
                               stop_words)

# words removed stop words
tidy_reviews <- tidy_reviews %>%
  drop_na %>%
  anti_join(custom_stop_words)
```

# Checking for top frequent words

```{r}
tidy_reviews %>%
  count(word, sort = TRUE)
```

# Build tf-idf tibble 

```{r}
## Build tf-idf tibble 
tidy_reviews_origin <- reviews %>% unnest_tokens(word, text)%>%
  count(review_number, word, sort = TRUE)
total_words_origin <- tidy_reviews_origin %>% group_by(review_number) %>%
  summarize(total = sum(n))

reviews_words_origin <- left_join(tidy_reviews_origin, total_words_origin)
review_tf_idf_origin <- reviews_words_origin %>%
  bind_tf_idf(word, review_number, n)
review_tf_idf_origin %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

# LDA

```{r}
# transfer tidy words to dtm form
review_dtm <- tidy_reviews %>% select(-sentiment,-lines) %>% 
  group_by(review_number) %>% count(word) %>% arrange(review_number,desc(n)) %>%
  cast_dtm(review_number, word,n)
```

```{r}
# find proper number of topics
result <- ldatuning::FindTopicsNumber(
  review_dtm,
  topics = seq(from = 6, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```

```{r}
# plot the graph of performance of different numbers of topics
ldatuning::FindTopicsNumber_plot(result)
```


```{r}
# LDA to separate to 15 topics
review_lda <- LDA(review_dtm, k=15,method = "Gibbs", control = list(seed = 1234))
review_lda
```

```{r}
# result of posterior distributions
tmResult <- posterior(review_lda)
# show the probability of each review on all 15 topics
theta <- tmResult$topics
# show the probability of each words on all 15 topics
beta <- tmResult$terms
```


```{r}
# rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")

# most probable topics in the collection
topicProportions <- colSums(theta) / nDocs(review_dtm)  # mean probabilities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
```


```{r}
countsOfPrimaryTopics <- rep(0, 15)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(review_dtm)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}

counts <- data.frame(names(countsOfPrimaryTopics),countsOfPrimaryTopics)

# graph of most popular topics
counts <- counts %>% rename(class=names.countsOfPrimaryTopics., 
                            count=countsOfPrimaryTopics) %>%
  arrange(desc(count))
ggplot(counts) +
  aes(x = reorder(class,count), y = count) +
  geom_col(fill = "#228B22") +
  labs(x = "film types by top frequent words", y = "counts", title="Counts of film types by top frequent words") +
  theme_minimal() +
  coord_flip()

```

```{r}
# assign each document of topic with the top frequent words and assign film types to each review
imdb_classed <- imdb
New_topicNames <- c('Family','Villian','Crime','Kids','Comedy','Horror','Action','Oscar','History','Female','Arts','Sitcom','Sci-fi','Dialogue','Musicals')
for (i in 1:nDocs(review_dtm)) {
  max_index <- which.max(theta[i, ])[[1]] # select topic distribution for document i
  index <- head(sort(theta[i,],decreasing = T), 3)
  # get first element position from ordered list
  imdb_classed$top_words[i] <- topicNames[max_index]
  imdb_classed$first_class[i] <- New_topicNames[as.numeric(names(index[1]))]
  imdb_classed$first_prob[i] <- index[[1]]
  imdb_classed$second_class[i] <- New_topicNames[as.numeric(names(index[2]))]
  imdb_classed$second_prob[i] <- index[[2]]
  imdb_classed$third_class[i] <- New_topicNames[as.numeric(names(index[3]))]
  imdb_classed$third_prob[i] <- index[[3]]
}
head(imdb_classed)
```

```{r}
write.csv(x = imdb_classed, file = "imdb_classed.csv")

```







